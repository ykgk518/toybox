# Learn Machine Learnining

# なぜ機械学習なのか？

一部のアプリケーション、人間がモデルをよく理解できているばあには、人間が決定ルールを記述することができる。
しかし、この方法には大きく2つの問題点がある。

* ある判断を行うためのロジックが個々のタスクのドメインに作郵のものになる。タスクが少しでも変わると、システム全体を書き直さなければならないかもしれない。

* ルールを設計するには、人間のエキスパートがどのように判断しているかを深く理解している必要がある。



# 教師あり学習


教師あり学習はある入力に対して特定の出力を予測したい場合で、入出力のペアの例が入手できる際に用いられます。
入出力のペアが訓練セットとなり、それから機械学習モデルを構築します。

目的は新しい見たことのないデータに対して正確な予測を行うこと。

## クラス分類と回帰
教師あり学習問題は**クラス分類**と**回帰**に大別することができます。

それぞれの目的は以下の通り。

* クラス分類 : あらかじめ定められた選択肢の中からクラスラべルを予測すること。
  * クラス分類には、**2クラス分類**と**多クラス分類**がある。

* 回帰 : 連続値の予測すること。


クラス分類か回帰かどうかを区別するには、出力に連続性があるかどうかを確認します。


## 汎化、過剰適合、適合不足について

汎化とは、訓練データに基づいて構築したモデルが未見のデータに対して正確に予想できる状態。

過剰適合とは

適合不足とは


## 教師あり機械学習アルゴリズム
個々の機械学習アルゴリズムについて記述すること
* 振る舞いについて
* メリット・デメリット
* モデルを構築する方法

### k-最近傍法(k-NN)

#### 概要
k-最近傍法の予測では訓練データセットの中から一番近い点(値)に着目する。
予測値は最も近い点に対する値をそのまま用いる。

訓練データに含まれる点(値)の中で予測対象のデータポイントに最も近いものを最近傍点という。
近傍点は1つとは限らず、任意の個数、k個の近傍点を考えることができる。

近傍点が複数の場合の予測は、多数決で予測値が決まる。

#### サンプルコード
scikit-learn用いて、分類器の生成、分類器の訓練、テストデータの予測、モデルの評価を行う。


```python
# サンプルデータ
import mglearn
from sklearn.model_selection import train_test_split
X, y = mglearn.datasets.make_forge()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)


# クラス分類器の生成
from sklearn.neighbors import KNeighborsClassifier
clf = KNeighborsClassifier(n_neighbors=3)

# 訓練セットを用いてクラス分類器を訓練
clf.fit(X_train, y_train)
予測値 : [1 0 1 0 1 0 0]
# テストデータの予測
print("{}".format(clf.predict(X_test)))

## モデルの汎化性能を評価
print("精度 : {}".format(clf.score(X_test, y_test)))
精度 : 0.8571428571428571
```

#### メリット・デメリット

メリット

* モデルが理解しやすい。
* パラメータ・距離尺度の調整をあまりしなくても十分に高い性能を持つ。

デメリット
* 訓練セット(特徴量とサンプル個数)が大きくなると予測は遅くなる。
* 大半の特徴量が0となる場合、性能が悪い。


### 線形モデル

#　## ナイーブベイズクラス分類器

### 決定木

### 決定木のアンサンブル法

### カーネル法を用いたサポートベクターマシン

### ニューラルネットワーク

## クラス分類器の不確実性推定



